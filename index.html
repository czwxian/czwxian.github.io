<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Zhiwen Chen</title>
  
  <meta name="author" content="Zhiwen Chen">
  <meta name="description" content="Hi! This is Zhiwen Chen. Welcome to my research portfolio. I am currently working on computer vision, machine learning and computer graphics.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Zhiwen, Chen, research, portfolio, computer vision, machine learning, computer graphics">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="personal/zhiwen.jpg">
  
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhiwen Chen (陈志文)</name>
              </p>
              <p>I'm currently working as a Staff Algorithm Engineer at <a href="https://www.alibabagroup.com/en/global/home" target="_blank">Alibaba Group</a> where I'm leading the PixelAI-Body Algorithm Team of <a href="https://tech.taobao.org/" target="_blank">Tao Technology</a>. Previously before 2017, I worked as a Video Analytic Researcher at <a href="https://www.trakomatic.com/" target="_blank">Trakomatic Pte. Ltd., Singapore</a> for several years.
              </p>
              <p>
                I received the B.E. degree in computer science from <a href="https://en.sjtu.edu.cn/" target="_blank">SJTU</a>, under the supervision of <a href="https://www.cs.sjtu.edu.cn/~fwu/" target="_blank">Prof. Fan Wu</a> in 2012 and the M.E. degree in computer science from <a href="https://www.nus.edu.sg/" target="_blank">NUS</a> in 2014.
              </p>
              <p style="text-align:center">
                <a href="mailto:czwxian@163.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.cn/injobs/in/chenzhiwen-05857148" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="personal/CV_ChenZhiwen_2022_en.pdf" target="_blank">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="personal/zhiwen.jpg">
            </td>
          </tr>
        </tbody></table>
		
		<hr>
		
		<!-- CV publications -->
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
			  <p>I'm interested in computer vision, in particular, 2D&3D Human Pose Estimation, 3D Human Body Reconstruction, Virtual Try-On, etc. Below are some highlighted publications.
			  </p>
            </td>
          </tr>
        </tbody></table>
		
	      
	      
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
			
		  <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="depth_paper/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9642435" target="_blank">
				<papertitle>High-Resolution Depth Maps Imaging via Attention-Based Hierarchical Multi-Modal Fusion</papertitle>
              </a>
              <br>
              <a href="https://ieeexplore.ieee.org/author/37088228328" target="_blank">Zhiwei Zhong</a>,
              <a href="https://ieeexplore.ieee.org/author/37406556000" target="_blank">Xianming Liu</a>,
              <a href="https://ieeexplore.ieee.org/author/37898149400" target="_blank">Junjun Jiang</a>,
              <a href="https://ieeexplore.ieee.org/author/37277017000" target="_blank">Debin Zhao</a>,
              <strong>Zhiwen Chen</strong>,
              <a href="https://ieeexplore.ieee.org/author/37271425200" target="_blank">Xiangyang Ji</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP) </em>, 2022 &nbsp <font color="red"><strong></strong></font>
              <br>
              <!--a href="clothvtonplus/" target="_blank">project page</a> /  -->
              <a href="https://arxiv.org/abs/2104.01530" target="_blank">arXiv</a> /
              <!--a href="https://www.youtube.com/watch?v=11uJkUQc614" target="_blank">video</a> /
              <!--a href="clothvtonplus/accv20_clothvton_slide.pdf" target="_blank">slide</a> / -->
              <a href="depth_paper/bibtex.txt" target="_blank">bibtex</a>
              <p></p>
              <p>We presented a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided depth map super-resolution.</p>
            </td>
          </tr>
			
			
      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="depth_paper/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9642435" target="_blank">
        <papertitle>High-Resolution Depth Maps Imaging via Attention-Based Hierarchical Multi-Modal Fusion</papertitle>
              </a>
              <br>
              <a href="https://ieeexplore.ieee.org/author/37088228328" target="_blank">Zhiwei Zhong</a>,
              <a href="https://ieeexplore.ieee.org/author/37406556000" target="_blank">Xianming Liu</a>,
              <a href="https://ieeexplore.ieee.org/author/37898149400" target="_blank">Junjun Jiang</a>,
              <a href="https://ieeexplore.ieee.org/author/37277017000" target="_blank">Debin Zhao</a>,
              <strong>Zhiwen Chen</strong>,
              <a href="https://ieeexplore.ieee.org/author/37271425200" target="_blank">Xiangyang Ji</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP) </em>, 2022 &nbsp <font color="red"><strong></strong></font>
              <br>
              <!--a href="clothvtonplus/" target="_blank">project page</a> /  -->
              <a href="https://arxiv.org/abs/2104.01530" target="_blank">arXiv</a> /
              <!--a href="https://www.youtube.com/watch?v=11uJkUQc614" target="_blank">video</a> /
              <!--a href="clothvtonplus/accv20_clothvton_slide.pdf" target="_blank">slide</a> / -->
              <a href="depth_paper/bibtex.txt" target="_blank">bibtex</a>
              <p></p>
              <p>We presented a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided depth map super-resolution.</p>
            </td>
          </tr>
		
        </tbody></table>
		
		
		<hr>
		
		<!-- awards -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Achievements</heading>
              <p> These include awards, challenges and competitions. 
              </p>
            </td>
          </tr>
        </tbody></table>
	
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>  
		  
		  <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/mpv_cvprw.gif" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
				<a href="https://vuhcs.github.io/" target="_blank">
				<papertitle>The 4th Look Into Person (LIP) Challenge - Track 3 Image-based Multi-pose Virtual Try-on Challenge</papertitle>
				</a>
			  <br>
              <a href="https://github.com/thaithanhtuan" target="_blank">Thai Thanh Tuan</a>,
			  <strong>Zhiwen Chen</strong>,
              <a href="https://square.seoultech.ac.kr/~icom/professor.html" target="_blank">Heejune Ahn</a>
				<br>
              <em>CVPR Workshop on Towards Human-Centric Image/Video Synthesis, and the 4th Look Into Person (LIP) Challenge, 2020</em> (<b>CVPR 2020</b>)
              <br>
              <b>2nd place winner</b>
              <br>
			  <a href="https://vuhcs.github.io/" target="_blank">workshop</a> /
              <a href="https://competitions.codalab.org/competitions/23471" target="_blank">challenge</a>
              <p></p>
              <p>Worked on design and implementation of a multi-pose guided image-based virtual try-on framework, trained on MPV dataset. We were evaluated 1st on SSIM metric, and 2nd on AMT study.</p>
            </td>
          </tr>
		  
		</tbody></table>
		  
		
		<hr>
		
		<!-- other CV related projects -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
              <p> These include courseworks, side projects, regionally published, and unpublished research works. 
              </p>
            </td>
          </tr>
        </tbody></table>
		
		  
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody> 
		  
		  
		  <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/st.png" alt="MS Thesis" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
				<a href="http://snut.dcollection.net/srch/srchDetail/200000372250" target="_blank">
                <papertitle>A Study on 3D reconstruction from clothing image and application to Virtual Try-On</papertitle>
              </a>
              <br>
		  <strong>Zhiwen Chen</strong>
		  <br>
              <em>MS Thesis, SeoulTech</em>
              <br>
              February 2021
              <br>
              <a href="http://snut.dcollection.net/srch/srchDetail/200000372250?localeParam=en" target="_blank">Manuscript</a>
              <p></p>
              <p>Master's thesis on the fashion-clothing based online virtual try-on project, exploring a hybrid approach to preserve the realism in the try-on output.</p>
            </td>
          </tr>
		  
		  
		  <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/3d_kijs.gif" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.koreascience.or.kr/article/JAKO201931765019080.page" target="_blank">
                <papertitle>3D Reconstruction of a Single Clothing Image and Its Application to Image-based Virtual Try-On</papertitle>
              </a> (Korean)
              <br>
              <a href="https://square.seoultech.ac.kr/~icom/professor.html" target="_blank">Heejune Ahn</a>,
              <strong>Zhiwen Chen</strong>
              <br>
              <em>Journal of the Korea Industrial Information Systems Research</em>, 2020 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://www.koreascience.or.kr/article/JAKO202031659968037.page" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=jYpDrPb6vdM" target="_blank">video</a> /
              <a href="https://github.com/ahnHeejune/smplvton01" target="_blank">code</a>
              <p></p>
              <p>We proposed a novel hybrid method for 3D clothing reconstruction and applying it to image-based virtual try-on (VTON) for fashion clothing, which generates realistically deformed try-on results.</p>
            </td>
          </tr>

		</tbody></table>
		
		
		<!-- Footnote -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody><tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template copied from <a style="font-size:small;" href="https://jonbarron.info" target="_blank">Jon Barron</a> and <a style="font-size:small;" href="https://leonidk.com/" target="_blank">Leonid Keselman</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
         

      </td>
    </tr>	  
  </table>
  
  <script src="//instant.page/5.1.0" type="module" integrity="sha384-by67kQnR+pyfy8yWP4kPO12fHKRLHZPfEsiSXR8u2IKcTdxD805MGUXBzVPnkLHw"></script>
  
</body>
    
