<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Zhiwen Chen</title>
  
  <meta name="author" content="Zhiwen Chen">
  <meta name="description" content="Hi! This is Zhiwen Chen. Welcome to my research portfolio. I am currently working on computer vision, machine learning and computer graphics.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Zhiwen, Chen, research, portfolio, computer vision, machine learning, computer graphics">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="personal/zhiwen.jpg">
  
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhiwen Chen (陈志文)</name>
              </p>
              <p>I'm currently working as a Staff Algorithm Engineer at <a href="https://www.alibabagroup.com/en/global/home" target="_blank">Alibaba Group</a> where I'm leading the PixelAI Algorithm Team of <a href="https://tech.taobao.org/" target="_blank">Tao Technology</a>. Previously before 2017, I worked as a Video Analytic Researcher at <a href="https://www.trakomatic.com/" target="_blank">Trakomatic Pte. Ltd., Singapore</a> for several years.
              </p>
              <p>
                I received the B.E. degree in computer science from <a href="https://en.sjtu.edu.cn/" target="_blank">SJTU</a>, under the supervision of <a href="https://www.cs.sjtu.edu.cn/~fwu/" target="_blank">Prof. Fan Wu</a> in 2012 and the M.E. degree in computer science from <a href="https://www.nus.edu.sg/" target="_blank">NUS</a> in 2014.
              </p>
              <p style="text-align:center">
                <a href="mailto:czwxian@163.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chenzhiwen-05857148" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="personal/CV_ChenZhiwen_2022_en.pdf" target="_blank">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="personal/zhiwen.jpg">
            </td>
          </tr>
        </tbody></table>
    
    <hr>
    
    <!-- CV publications -->
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
        <p>I'm interested in computer vision, in particular, 2D&3D Human Pose Estimation, 3D Human Body Reconstruction, Virtual Try-On, etc. Below are some highlighted publications.
        </p>
            </td>
          </tr>
        </tbody></table>
    
        
        
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="image1.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.11184" target="_blank">
        <papertitle>MVP-Human Dataset for 3D Clothed Human Avatar Reconstruction from Multiple Frames</papertitle>
              </a>
              <br>
              Xiangyu Zhu,
              Tingting Liao,
              Xiaomei Zhang,
              Jiangjing Lyu,
              <strong>Zhiwen Chen</strong>,
              Yunfeng Wang,
              Kan Guo,
              Qiong Cao,
              Stan Z. Li,
              Zhen Lei
              <br>
              <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM) </em>, 2022 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2204.11184" target="_blank">arXiv</a>
              <p></p>
              <p>We contribute a large-scale dataset, MVP-Human (Multi-View and multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans in different poses and 8-view images for each pose.</p>
            </td>
          </tr>    
      
      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="depth_paper/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9642435" target="_blank">
        <papertitle>High-Resolution Depth Maps Imaging via Attention-Based Hierarchical Multi-Modal Fusion</papertitle>
              </a>
              <br>
              Zhiwei Zhong,
              Xianming Liu,
              Junjun Jiang,
              Debin Zhao,
              <strong>Zhiwen Chen</strong>,
              Xiangyang Ji
              <br>
              <em>IEEE Transactions on Image Processing (TIP) </em>, 2022 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2104.01530" target="_blank">arXiv</a> /
              <a href="depth_paper/bibtex.txt" target="_blank">bibtex</a>
              <p></p>
              <p>We presented a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided depth map super-resolution.</p>
            </td>
          </tr>
      
      
      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="decoding_paper/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9662665" target="_blank">
        <papertitle>NL-CALIC Soft Decoding Using Strict Constrained Wide-Activated Recurrent Residual Network</papertitle>
              </a>
              <br>
              Yi Niu,
              Chang Liu,
              Mingming Ma,
              Fu Li,
              <strong>Zhiwen Chen</strong>,
              Guangming Shi
              <br>
              <em>IEEE Transactions on Image Processing (TIP) </em>, 2022 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://github.com/dota-109/WRSD" target="_blank">code</a> /
              <a href="decoding_paper/bibtex.txt" target="_blank">bibtex</a>
              <p></p>
              <p>We proposed a Wide-activated Recurrent structure with a normalized Tanh activated strategy for Soft-Decoding (WRSD).</p>
            </td>
          </tr>

      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="skeleton_paper/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9432856" target="_blank">
        <papertitle>Adaptive Linear Span Network for Object Skeleton Detection</papertitle>
              </a>
              <br>
              Chang Liu,
              Yunjie Tian,
              <strong>Zhiwen Chen</strong>,
              Jianbin Jiao,
              Qixiang Ye
              <br>
              <em>IEEE Transactions on Image Processing (TIP) </em>, 2021 &nbsp <font color="red"><strong></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2011.03972" target="_blank">arXiv</a> /
              <a href="https://github.com/sunsmarterjie/SDL-Skeleton" target="_blank">code</a> /
              <a href="skeleton_paper/bibtex.txt" target="_blank">bibtex</a>
              <p></p>
              <p>We proposed adaptive linear span network (AdaLSN), and automatically configured and integrated scale-aware features for object skeleton detection.</p>
            </td>
          </tr>
    
        </tbody></table>
    
    
    <hr>
    
    <!-- awards -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Achievements</heading>
              <p> These include workshops, challenges and awards. 
              </p>
            </td>
          </tr>
        </tbody></table>
  
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>  

      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="image.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/wcpa2022/home" target="_blank">
        <papertitle>1st International Workshop and Challenge on People Analysis: From Face, Body and Fashion to 3D Virtual Avatars</papertitle>
        </a>
        <br>
              <strong>Zhiwen Chen</strong> (Challenge Main Organizer)
        <br>
              <em>Workshop and challenge on ECCV</em>, 2022
              <br>
              <a href="https://tianchi.aliyun.com/competition/entrance/531958/introduction" target="_blank">challenge</a>
              <p></p>
              <p>We contribute a large-scale dataset, MVP-Human (Multi-View and Multi-Pose 3D Human), which contains 250 subjects. Each subject has 15 type of different poses. Each pose contains 8-view RGB images.</p>
            </td>
          </tr>
      
      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="dlgc/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/dlgc-workshop-cvpr2022" target="_blank">
        <papertitle>The Fourth Workshop on Deep Learning for Geometric Computing</papertitle>
        </a>
        <br>
              Zixuan Huang,
              Yunfeng Wang,
              <strong>Zhiwen Chen</strong>
        <br>
              <em>Workshop and challenge on CVPR</em>, 2022
              <br>
              <b>1st place winner of Pixel SkelNetOn Track</b>
              <br>
        <a href="https://sites.google.com/view/dlgc-workshop-cvpr2022/home" target="_blank">workshop</a> /
              <a href="https://competitions.codalab.org/competitions/21169" target="_blank">challenge</a> /
              <a href="https://arxiv.org/abs/2205.12066" target="_blank">arXiv</a>
              <p></p>
              <p>We proposed an attention-based model called Context Attention Network (CANet), which integrates the context extraction module in a UNet architecture, can effectively improve the network’s ability to extract the skeleton pixels.</p>
            </td>
          </tr>

      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="somof/image.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
        <a href="https://somof.stanford.edu/" target="_blank">
        <papertitle>1st Workshop, Benchmark and Challenge on Human Trajectory and Pose Dynamics Forecasting in the Wild</papertitle>
        </a>
        <br>
              Chenxi Wang,
              Yunfeng Wang,
              Zixuan Huang,
              <strong>Zhiwen Chen</strong>
        <br>
              <em>Workshop and challenge on ICCV</em>, 2021
              <br>
              <b>1st place winner of PoseTrack and 3DPW datasets</b>
              <br>
        <a href="https://somof.stanford.edu/workshops/iccv21" target="_blank">workshop</a> /
              <a href="https://somof.stanford.edu/results" target="_blank">challenge</a> /
              <a href="https://arxiv.org/abs/2110.07495" target="_blank">arXiv</a>
              <p></p>
              <p>We established a simple but effective baseline for single human motion forecasting without visual and social information. We were evaluated 1st place on both PoseTrack dataset and 3DPW dataset.</p>
            </td>
          </tr>
      
    </tbody></table>
      
    
    <!--<hr> -->
    
    <!-- other CV related projects -->
    <!--<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
              <p> These include courseworks, side projects, regionally published, and unpublished research works. 
              </p>
            </td>
          </tr>
        </tbody></table>
    
      
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody> 
      
      
      <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/st.png" alt="MS Thesis" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
        <a href="http://snut.dcollection.net/srch/srchDetail/200000372250" target="_blank">
                <papertitle>A Study on 3D reconstruction from clothing image and application to Virtual Try-On</papertitle>
              </a>
              <br>
      <strong>Zhiwen Chen</strong>
      <br>
              <em>MS Thesis, SeoulTech</em>
              <br>
              February 2021
              <br>
              <a href="http://snut.dcollection.net/srch/srchDetail/200000372250?localeParam=en" target="_blank">Manuscript</a>
              <p></p>
              <p>Master's thesis on the fashion-clothing based online virtual try-on project, exploring a hybrid approach to preserve the realism in the try-on output.</p>
            </td>
          </tr>
    </tbody></table>-->
    
    
    <!-- Footnote -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody><tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template copied from <a style="font-size:small;" href="https://jonbarron.info" target="_blank">Jon Barron</a> and <a style="font-size:small;" href="https://minar09.github.io/" target="_blank">Matiur Rahman Minar</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
         

      </td>
    </tr>   
  </table>
  
  <script src="//instant.page/5.1.0" type="module" integrity="sha384-by67kQnR+pyfy8yWP4kPO12fHKRLHZPfEsiSXR8u2IKcTdxD805MGUXBzVPnkLHw"></script>
  
</body>
